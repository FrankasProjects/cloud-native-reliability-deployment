@inproceedings{10.1109/CCGRID.2017.58,
author = {Gonz\'{a}lez, Patricia and Pardo, Xo\'{a}n C. and Penas, David R. and Teijeiro, Diego and Banga, Julio R. and Doallo, Ram\'{o}n},
title = {Using the Cloud for Parameter Estimation Problems: Comparing Spark vs MPI with a Case-Study},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.58},
doi = {10.1109/CCGRID.2017.58},
abstract = {Systems biology is an emerging approach focused in generating new knowledge about complex biological systems by combining experimental data with mathematical modeling and advanced computational techniques. Many problems in this field are extremely challenging and require substantial supercomputing resources to be solved. This is the case of parameter estimation in large-scale nonlinear dynamic systems biology models. Recently, Cloud Computing has emerged as a new paradigm for on-demand delivery of computing resources. However, scientific computing community has been quite hesitant in using the Cloud, simply because traditional programming models do not fit well with the new paradigm, and the earliest cloud programming models do not allow most scientific computations being efficiently run in the Cloud. In this paper we explore and compare two distributed computing models: the MPI (message-passing interface) model, that is high-performance oriented, and the Spark model, which is throughput oriented but outperforms other cloud programming solutions adding improved support for iterative algorithms through in-memory computing. The performance of a very well known metaheuristic, the Differential Evolution algorithm, has been thoroughly assessed using a challenging parameter estimation problem from the domain of computational systems biology. The experiments have been carried out both in a local cluster and in the Microsoft Azure public cloud, allowing performance and cost evaluation for both infrastructures.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {797–806},
numpages = {10},
keywords = {Microsoft Azure, Spark, Cloud Computing, MPI, Differential Evolution, Parallel Metaheuristics},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1109/CCGrid.2012.46,
author = {Villegas, David and Antoniou, Athanasios and Sadjadi, Seyed Masoud and Iosup, Alexandru},
title = {An Analysis of Provisioning and Allocation Policies for Infrastructure-as-a-Service Clouds},
year = {2012},
isbn = {9780769546919},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGrid.2012.46},
doi = {10.1109/CCGrid.2012.46},
abstract = {Today, many commercial and private cloud computing providers offer resources for leasing under the infrastructure as a service (IaaS) paradigm. Although an abundance of mechanisms already facilitate the lease and use of single infrastructure resources, to complete multi-job workloads IaaS users still need to select adequate provisioning and allocation policies to instantiate resources and map computational jobs to them. While such policies have been studied in the past, no experimental investigation in the context of clouds currently exists that considers them jointly. In this paper we present a comprehensive and empirical performance-cost analysis of provisioning and allocation policies in IaaS clouds. We first introduce a taxonomy of both types of policies, based on the type of information used in the decision process, and map to this taxonomy eight provisioning and four allocation policies. Then, we analyze the performance and cost of these policies through experimentation in three clouds, including Amazon EC2. We show that policies that dynamically provision and/or allocate resources can achieve better performance and cost. Finally, we also look at the interplay between provisioning and allocation, for which we show preliminary results.},
booktitle = {Proceedings of the 2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
pages = {612–619},
numpages = {8},
keywords = {Cloud computing, Scheduling, allocation policies, empirical performance analysis, provisioning policies},
series = {CCGRID '12}
}

@inproceedings{10.1145/1952682.1952691,
author = {Cecchet, Emmanuel and Singh, Rahul and Sharma, Upendra and Shenoy, Prashant},
title = {Dolly: Virtualization-Driven Database Provisioning for the Cloud},
year = {2011},
isbn = {9781450306874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1952682.1952691},
doi = {10.1145/1952682.1952691},
abstract = {Cloud computing platforms are becoming increasingly popular for e-commerce applications that can be scaled on-demand in a very cost effective way. Dynamic provisioning is used to autonomously add capacity in multi-tier cloud-based applications that see workload increases. While many solutions exist to provision tiers with little or no state in applications, the database tier remains problematic for dynamic provisioning due to the need to replicate its large disk state. In this paper, we explore virtual machine (VM) cloning techniques to spawn database replicas and address the challenges of provisioning shared-nothing replicated databases in the cloud. We argue that being able to determine state replication time is crucial for provisioning databases and show that VM cloning provides this property. We propose Dolly, a database provisioning system based on VM cloning and cost models to adapt the provisioning policy to the cloud infrastructure specifics and application requirements. We present an implementation of Dolly in a commercial-grade replication middleware and evaluate database provisioning strategies for a TPC-W workload on a private cloud and on Amazon EC2. By being aware of VM-based state replication cost, Dolly can solve the challenge of automated provisioning for replicated databases on cloud platforms.},
booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {51–62},
numpages = {12},
keywords = {virtualization, autonomic provisioning, database},
location = {Newport Beach, California, USA},
series = {VEE '11}
}

@article{10.1145/2007477.1952691,
author = {Cecchet, Emmanuel and Singh, Rahul and Sharma, Upendra and Shenoy, Prashant},
title = {Dolly: Virtualization-Driven Database Provisioning for the Cloud},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/2007477.1952691},
doi = {10.1145/2007477.1952691},
abstract = {Cloud computing platforms are becoming increasingly popular for e-commerce applications that can be scaled on-demand in a very cost effective way. Dynamic provisioning is used to autonomously add capacity in multi-tier cloud-based applications that see workload increases. While many solutions exist to provision tiers with little or no state in applications, the database tier remains problematic for dynamic provisioning due to the need to replicate its large disk state. In this paper, we explore virtual machine (VM) cloning techniques to spawn database replicas and address the challenges of provisioning shared-nothing replicated databases in the cloud. We argue that being able to determine state replication time is crucial for provisioning databases and show that VM cloning provides this property. We propose Dolly, a database provisioning system based on VM cloning and cost models to adapt the provisioning policy to the cloud infrastructure specifics and application requirements. We present an implementation of Dolly in a commercial-grade replication middleware and evaluate database provisioning strategies for a TPC-W workload on a private cloud and on Amazon EC2. By being aware of VM-based state replication cost, Dolly can solve the challenge of automated provisioning for replicated databases on cloud platforms.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {51–62},
numpages = {12},
keywords = {virtualization, autonomic provisioning, database}
}

@proceedings{10.1145/1869692,
title = {HPDGIS '10: Proceedings of the ACM SIGSPATIAL International Workshop on High Performance and Distributed Geographic Information Systems},
year = {2010},
isbn = {9781450304320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {High performance computing and distributed systems have become prominent elements in the landscape of computing and information technologies. High performance and distributed GIS (HPDGIS) have emerged as a growing area of theoretical and applied research. This growth is driven by geospatial problems in numerous fields that are increasingly computationally intensive and require collaboration support. Efficient handling of massive spatial databases, shared and role-based access to distributed data, and high end computing services are fundamental to the near-real-time response times required for many GIS and associated decision support applications.The initial feasibility and tremendous potential of HPDGIS have recently been demonstrated by exploiting rapidly developing cyberinfrastructure capabilities. It is therefore important to bring together researchers and practitioners to map out fundamental research areas centered on HPDGIS and its tight connections to advances in high performance computing, distributed systems, and associated GIS and spatial analysis applications and this inaugural ACM SIGSPATIAL HPDGIS 2010 is designed to do just that. This proceeding contains papers selected for publication and presentation, to this HPDGIS'10 workshop, held at San Jose, California, USA on November 2, 2010 in conjunction with the 18th ACM SIGSPATIAL International Conference on Advances in Geographic Information System.This year's program also features an outstanding keynote talk titled as "High Performance Computing with Spatial Datasets" by Dr. Shashi Shekhar from the University of Minnesota.The workshop attracted research papers on a number of HPDGIS themes including data intensive GIS, parallel processing algorithms for GIS problems, GIS based on cloud computing, service-oriented GIS, and spatial middleware. Research papers: "A MapReduce Approach to Gi*(d) Spatial Statistic" and "Spatial Scene Similarity Assessment on Hadoop" illustrate the use of the map-reduce framework to resolve two typical data-intensive problems in GIS and spatial analysis. In "Towards Personal High- Performance Geospatial Computing (HPC-G): Perspectives and a Case Study", the author advocates the use of a low cost personal HPDGIS environment developed by using parallel computing capability afforded by Graphic Processing Unit architecture. A theoretical framework for modeling the cost of a distributed service on cloud is discussed in "A Cost Model for Distributed Coverage Processing Services", while the paper titled as "Cloud Computing for Geosciences: Deployment of GEOSS Clearinghouse on Amazon's EC2" experimentally demonstrates the use of cloud computing for GIS and spatial analysis. "High Performance Computing: Fundamental Research Challenges in Service Oriented GIS" identifies a set of fundamental research challenges for the realization of service-oriented GIS. "A Distributed Resource Broker for Spatial Middleware Using Adaptive Space-Filling Curve" presents a spatial middleware component to enable HPDGIS applications by exploiting computational capabilities of cyberinfrastructure.},
location = {San Jose, California}
}

@inproceedings{10.1145/2755979.2755986,
author = {Gupta, Abhishek},
title = {Efficient High Performance Computing in the Cloud: Keynote Talk},
year = {2015},
isbn = {9781450335737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755979.2755986},
doi = {10.1145/2755979.2755986},
abstract = {The advantages of pay-as-you-go model, elasticity, and the flexibility and customization offered by virtualization make cloud computing an attractive economical option for meeting the needs of some HPC users. However, there is a mismatch between current cloud environments and HPC requirements. HPC is performance-oriented, whereas clouds are cost and resource-utilization oriented. The poor interconnect and I/O performance in cloud, HPC-agnostic cloud schedulers, and the inherent heterogeneity and multi-tenancy in cloud are some bottlenecks for HPC in cloud. This means that the tremendous potential of cloud for both HPC users and providers remains underutilized. In this talk, we will go beyond the common research question: "what is the performance of HPC in cloud?" and present our research on "how can we perform cost-effective and efficient HPC in cloud?" To this end, we will present the complementary approach of making clouds HPC-aware, and HPC runtime system cloud-aware. Through comprehensive HPC performance and cost analysis, HPC-aware VM placement, interference-aware VM consolidation, Multi-dimensional Online Bin Packing, malleable jobs, and cloud-aware HPC load balancing, we demonstrate significant benefits for both: users and cloud providers in terms of cost (up to 60%), performance (up to 45%), and throughput (up to 32%).},
booktitle = {Proceedings of the 8th International Workshop on Virtualization Technologies in Distributed Computing},
pages = {1},
numpages = {1},
keywords = {malleable jobs, scheduling, hpc in cloud, performance analysis, vm placement},
location = {Portland, Oregon, USA},
series = {VTDC '15}
}

@inproceedings{10.1145/3293320.3293321,
author = {Cardwell, David and Song, Fengguang},
title = {An Extended Roofline Model with Communication-Awareness for Distributed-Memory HPC Systems},
year = {2019},
isbn = {9781450366328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293320.3293321},
doi = {10.1145/3293320.3293321},
abstract = {Performance modeling of parallel applications on distributed memory systems is a challenging task due to the effects of CPU speed, memory access time, and communication cost. In this paper, we propose a simple and intuitive graphical model, which extends the widely used Roofline performance model to include the communication cost in addition to the memory access time and the peak CPU performance. This new performance model inherits the simplicity of the original Roofline model and enables performance evaluation on a third dimension of communication performance. Such a model will greatly facilitate and expedite the analysis, development and optimization of parallel programs on high-end computer systems. We empirically validate the extended new Roofline model usingfl oating-point-computation-bound, memory-bound, and communication-bound applications. Three distinct high-end computing platforms have been tested: 1) high performance computing (HPC) systems, 2) high throughput computing systems, and 3) cloud computing systems. Our experimental results with four different parallel applications show that the new model can approximately evaluate the performance of different programs on various distributed-memory systems. Furthermore, the extended new model is able to provide insight into how the problem size can affect the upper bound performance of parallel applications, which is a special property revealed by the new dimension of communication cost analysis.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {26–35},
numpages = {10},
keywords = {performance prediction, Roofline model, visualization},
location = {Guangzhou, China},
series = {HPC Asia 2019}
}

@inproceedings{10.1145/2390021.2390025,
author = {Curino, Carlo A. and Difallah, Djellel E. and Pavlo, Andrew and Cudre-Mauroux, Philippe},
title = {Benchmarking OLTP/Web Databases in the Cloud: The OLTP-Bench Framework},
year = {2012},
isbn = {9781450317085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390021.2390025},
doi = {10.1145/2390021.2390025},
abstract = {Benchmarking is a key activity in building and tuning data management systems, but the lack of reference workloads and a common platform makes it a time consuming and painful task. The need for such a tool is heightened with the advent of cloud computing--with its pay-per-use cost models, shared multi-tenant infrastructures, and lack of control on system configuration. Benchmarking is the only avenue for users to validate the quality of service they receive and to optimize their deployments for performance and resource utilization. In this talk, we present our experience in building several adhoc benchmarking infrastructures for various research projects targeting several OLTP DBMSs, ranging from traditional relational databases, main-memory distributed systems, and cloud-based scalable architectures. We also discuss our struggle to build meaningful micro-benchmarks and gather workloads representative of real-world applications to stress-test our systems. This experience motivates the OLTP-Bench project, a batteries-included benchmarking infrastructure designed for and tested on several relational DBMSs and cloud-based database-as-a-service (DBaaS) offerings. OLTP-Bench is capable of controlling transaction rate, mixture, and workload skew dynamically during the execution of an experiment, thus allowing the user to simulate a multitude of practical scenarios that are typically hard to test (e.g., time-evolving access skew). Moreover, the infrastructure provides an easy way to monitor performance and resource consumption of the database under test. We also introduce the ten included workloads, derived from either synthetic micro benchmarks, popular benchmarks, and real world applications, and how they can be used to investigate various performance and resource-consumption characteristics of a data management system. We showcase the effectiveness of our benchmarking infrastructure and the usefulness of the workloads we selected by reporting sample results from hundreds of side-byside comparisons on popular DBMSs and DBaaS offerings.},
booktitle = {Proceedings of the Fourth International Workshop on Cloud Data Management},
pages = {17–20},
numpages = {4},
keywords = {benchmarking, databases, oltp},
location = {Maui, Hawaii, USA},
series = {CloudDB '12}
}

@inproceedings{10.1145/3590837.3590922,
author = {M, Saravanan and S, Shiva Prasad},
title = {A Blockchain-Based, Distributed, Self Hosted And End To End Encrypted Cloud Storage System},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590922},
doi = {10.1145/3590837.3590922},
abstract = {Cloud computing is fast taking over due to its convenience and greater safety. You may access your files anytime you need them by using the cloud. Having a large following makes things simpler for consumers. Computing makes phase, programming, and system structure all possible. With the aid of these components, increased cloud-based information and profits might be obtained. It is great to plan and arrange work based on data. The most difficult issue for people with a background in logical thinking is planning work procedures to achieve customer service goals while keeping expenses in control. Although leveraging cloud storage to reduce expenses has been suggested, doing so can be difficult. Although leveraging cloud storage to reduce expenses has been suggested, doing so can be difficult. To handle cloud resources efficiently, a modular infrastructure is needed. Utilizing standards and calculations, parallel resource and service management is maximized in the cloud. Using different work flows to structure work processes is the most entertaining activity in cloud computing. Timing and price have an influence on service quality (tasks). Workflow-based relocation of virtual machines is more effective. NP-hard algorithms for subset and choice issues. Making a choice allows the server to save time and money. PSO and GWO interactions that are advantageous. In this undertaking, both time and money are considerations. The new approach should be used. The study affects the validity of process parameters. intuition with a convex shape. utilizing the PEFT technique. GWO analyses the time and money spent on cloud processes. It is suggested that VMs be optimized as hybrid, both locally and globally. heuristic algorithm based on PEFT. Optimization reduces the likelihood of making a mistake right away. The Grey Wolf Optimization and Floral pollination algorithm outperforms genetic and flower pollination techniques. Biomimicry is compared with swarm intelligence. For our analysis, we use LIGO, GENOME, CYBER SHAKE, and SIPHT. The difficulty and quantity of the jobs have an impact on workflow. A bio-inspired GA, GWO, and FPA are used in the optimization process. In an experimental arrangement, time and cost analysis for two to twenty VMs and workflows may be done. Compared to FPA with PEFT, GWO requires less time and money. In hybrid optimization, GWO and FPA are combined. In this project, efficiency and speed are highly valued. GWO optimizes VM globally, whereas FPA concentrates on local enhancements. FPA GA uses the collective wisdom of the group to identify correlations. As labor prices grow, more virtual machines (VMs) are employed for processing and tasks. Wait times drop and costs rise. Local and global optimization have an impact on virtual machine (VM) and compute time. ACO and PSO are used to accomplish local and global optimization, however employing them requires more time.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {85},
numpages = {7},
keywords = {Cryptography, Cloud storage, Blockchain, Peer to Peer Network, IPFS},
location = {Jaipur, India},
series = {ICIMMI '22}
}

